{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pacakge & Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# import labels for gudhi shapes\n",
    "gudhi_shape_labels = np.genfromtxt('Gudhi Shape Dataset/shape_labels.csv', delimiter=',', skip_header=1)\n",
    "gudhi_shape_labels = gudhi_shape_labels.astype(int)[:,2]\n",
    "print(len(gudhi_shape_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected 2000 samples.\n",
      "Shape of laplacians: (2000, 1000, 1000)\n",
      "Shape of VR persistence images: (2000, 100, 100)\n",
      "Shape of abstract persistence images: (2000, 100, 100)\n",
      "Shape of selected labels: (2000,)\n"
     ]
    }
   ],
   "source": [
    "num_samples = 2000 # currently set to full dataset\n",
    "\n",
    "# Generate random indices\n",
    "random_indices = np.random.choice(len(gudhi_shape_labels), size=num_samples, replace=False)\n",
    "base = 'Gudhi Shape Dataset/'\n",
    "# Select the corresponding data and labels\n",
    "gudhi_laplacians = []\n",
    "gudhi_vr_persistence_images = []\n",
    "gudhi_abstract_persistence_images = []\n",
    "gudhi_selected_labels = []\n",
    "\n",
    "for i in random_indices:\n",
    "    gudhi_laplacians.append(np.genfromtxt(f'{base}/shape_{i}_laplacian.csv', delimiter=',', skip_header=0))\n",
    "    gudhi_vr_persistence_images.append(np.genfromtxt(f'{base}/shape_{i}_vr_persistence_image.csv', delimiter=',', skip_header=0))\n",
    "    gudhi_abstract_persistence_images.append(np.genfromtxt(f'{base}/shape_{i}_abstract_persistence_image.csv', delimiter=',', skip_header=0))\n",
    "    gudhi_selected_labels.append(gudhi_shape_labels[i])\n",
    "\n",
    "# Convert selected labels to NumPy array\n",
    "gudhi_selected_labels = np.array(gudhi_selected_labels)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Randomly selected {num_samples} samples.\")\n",
    "print(f\"Shape of laplacians: {np.array(gudhi_laplacians).shape}\")\n",
    "print(f\"Shape of VR persistence images: {np.array(gudhi_vr_persistence_images).shape}\")\n",
    "print(f\"Shape of abstract persistence images: {np.array(gudhi_abstract_persistence_images).shape}\")\n",
    "print(f\"Shape of selected labels: {gudhi_selected_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    }
   ],
   "source": [
    "# import labels for medical shapes\n",
    "medical_shape_labels = np.genfromtxt('Medical Dataset/shape_labels.csv', delimiter=',', skip_header=1)\n",
    "medical_shape_labels = medical_shape_labels.astype(int)[:,2]\n",
    "print(len(medical_shape_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected 162 samples.\n",
      "Shape of laplacians: (162, 1000, 1000)\n",
      "Shape of VR persistence images: (162, 100, 100)\n",
      "Shape of abstract persistence images: (162, 100, 100)\n",
      "Shape of selected labels: (162,)\n"
     ]
    }
   ],
   "source": [
    "num_samples = 162 # currently set to full dataset\n",
    "\n",
    "# Generate random indices\n",
    "random_indices = np.random.choice(len(medical_shape_labels), size=num_samples, replace=False)\n",
    "base = 'Medical Dataset/'\n",
    "# Select the corresponding data and labels\n",
    "medical_laplacians = []\n",
    "medical_vr_persistence_images = []\n",
    "medical_abstract_persistence_images = []\n",
    "medical_selected_labels = []\n",
    "\n",
    "for i in random_indices:\n",
    "    medical_laplacians.append(np.genfromtxt(f'{base}/shape_{i}_laplacian.csv', delimiter=',', skip_header=0))\n",
    "    medical_vr_persistence_images.append(np.genfromtxt(f'{base}/shape_{i}_vr_persistence_image.csv', delimiter=',', skip_header=0))\n",
    "    medical_abstract_persistence_images.append(np.genfromtxt(f'{base}/shape_{i}_abstract_persistence_image.csv', delimiter=',', skip_header=0))\n",
    "    medical_selected_labels.append(medical_shape_labels[i])\n",
    "\n",
    "# Convert selected labels to NumPy array\n",
    "medical_selected_labels = np.array(medical_selected_labels)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Randomly selected {num_samples} samples.\")\n",
    "print(f\"Shape of laplacians: {np.array(medical_laplacians).shape}\")\n",
    "print(f\"Shape of VR persistence images: {np.array(medical_vr_persistence_images).shape}\")\n",
    "print(f\"Shape of abstract persistence images: {np.array(medical_abstract_persistence_images).shape}\")\n",
    "print(f\"Shape of selected labels: {medical_selected_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = [torch.tensor(d, dtype=torch.float32).unsqueeze(0) for d in data]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplacians Train data size: 1600\n",
      "Laplacians Validation data size: 281\n",
      "Laplacians Test data size: 281\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8\n",
    "valid_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Split Gudhi Laplacians\n",
    "train_data_gudhi_laplacians, test_data_gudhi_laplacians, train_labels_gudhi, test_labels_gudhi = train_test_split(\n",
    "    gudhi_laplacians, gudhi_selected_labels, test_size=(1 - train_ratio), random_state=42\n",
    ")\n",
    "valid_data_gudhi_laplacians, test_data_gudhi_laplacians, valid_labels_gudhi, test_labels_gudhi = train_test_split(\n",
    "    test_data_gudhi_laplacians, test_labels_gudhi, test_size=(test_ratio / (valid_ratio + test_ratio)), random_state=42\n",
    ")\n",
    "\n",
    "# Split Medical Laplacians (valid/test split only, since all are used for validation/testing)\n",
    "valid_data_medical_laplacians, test_data_medical_laplacians, valid_labels_medical, test_labels_medical = train_test_split(\n",
    "    medical_laplacians, medical_selected_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Combine validation sets for Laplacians and labels\n",
    "valid_laplacians = valid_data_gudhi_laplacians + valid_data_medical_laplacians\n",
    "valid_labels = np.concatenate((valid_labels_gudhi, valid_labels_medical))\n",
    "\n",
    "# Combine test sets for completeness\n",
    "test_laplacians = test_data_gudhi_laplacians + test_data_medical_laplacians\n",
    "test_labels = np.concatenate((test_labels_gudhi, test_labels_medical))\n",
    "\n",
    "# Print a summary\n",
    "print(f\"Laplacians Train data size: {len(train_data_gudhi_laplacians)}\")\n",
    "print(f\"Laplacians Validation data size: {len(valid_laplacians)}\")\n",
    "print(f\"Laplacians Test data size: {len(test_laplacians)}\")\n",
    "\n",
    "\n",
    "# Split VR Persistence Images\n",
    "train_data_gudhi_vr_persistence_images, test_data_gudhi_vr_persistence_images, train_labels_gudhi_check, test_labels_gudhi_check = train_test_split(\n",
    "    gudhi_vr_persistence_images, gudhi_selected_labels, test_size=(1 - train_ratio), random_state=42\n",
    ")\n",
    "valid_data_gudhi_vr_persistence_images, test_data_gudhi_vr_persistence_images, valid_labels_gudhi_check, test_labels_gudhi_check = train_test_split(\n",
    "    test_data_gudhi_vr_persistence_images, test_labels_gudhi_check, test_size=(test_ratio / (valid_ratio + test_ratio)), random_state=42\n",
    ")\n",
    "\n",
    "# Sanity check: Ensure indices for labels are consistent\n",
    "assert np.array_equal(train_labels_gudhi, train_labels_gudhi_check), \"Train labels do not match for Laplacians and VR persistence images!\"\n",
    "assert np.array_equal(test_labels_gudhi, test_labels_gudhi_check), \"Test labels do not match for Laplacians and VR persistence images!\"\n",
    "assert np.array_equal(valid_labels_gudhi, valid_labels_gudhi_check), \"Validation labels do not match for Laplacians and VR persistence images!\"\n",
    "\n",
    "# Split Medical VR Persistence Images (valid/test split only, since all are used for validation/testing)\n",
    "valid_data_medical_vr_persistence_images, test_data_medical_vr_persistence_images, valid_labels_medical_check, test_labels_medical_check = train_test_split(\n",
    "    medical_vr_persistence_images, medical_selected_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Sanity check: Ensure medical label splits are consistent\n",
    "assert np.array_equal(valid_labels_medical, valid_labels_medical_check), \"Validation labels do not match for Medical Laplacians and VR persistence images!\"\n",
    "assert np.array_equal(test_labels_medical, test_labels_medical_check), \"Test labels do not match for Medical Laplacians and VR persistence images!\"\n",
    "\n",
    "\n",
    "# Combine validation sets for VR Persistence Images and labels\n",
    "valid_vr_persistence_images = valid_data_gudhi_vr_persistence_images + valid_data_medical_vr_persistence_images\n",
    "valid_vr_labels = np.concatenate((valid_labels_gudhi, valid_labels_medical))\n",
    "\n",
    "# Combine test sets for VR Persistence Images and labels\n",
    "test_vr_persistence_images = test_data_gudhi_vr_persistence_images + test_data_medical_vr_persistence_images\n",
    "test_vr_labels = np.concatenate((test_labels_gudhi, test_labels_medical))\n",
    "\n",
    "\n",
    "# Split Abstract Persistence Images\n",
    "train_data_gudhi_abstract_persistence_images, test_data_gudhi_abstract_persistence_images, train_labels_gudhi_check, test_labels_gudhi_check = train_test_split(\n",
    "    gudhi_abstract_persistence_images, gudhi_selected_labels, test_size=(1 - train_ratio), random_state=42\n",
    ")\n",
    "valid_data_gudhi_abstract_persistence_images, test_data_gudhi_abstract_persistence_images, valid_labels_gudhi_check, test_labels_gudhi_check = train_test_split(\n",
    "    test_data_gudhi_abstract_persistence_images, test_labels_gudhi_check, test_size=(test_ratio / (valid_ratio + test_ratio)), random_state=42\n",
    ")\n",
    "\n",
    "# Sanity check: Ensure indices for labels are consistent\n",
    "assert np.array_equal(train_labels_gudhi, train_labels_gudhi_check), \"Train labels do not match for Laplacians and Abstract persistence images!\"\n",
    "assert np.array_equal(test_labels_gudhi, test_labels_gudhi_check), \"Test labels do not match for Laplacians and Abstract persistence images!\"\n",
    "assert np.array_equal(valid_labels_gudhi, valid_labels_gudhi_check), \"Validation labels do not match for Laplacians and Abstract persistence images!\"\n",
    "\n",
    "# Split Medical Abstract Persistence Images (valid/test split only, since all are used for validation/testing)\n",
    "valid_data_medical_abstract_persistence_images, test_data_medical_abstract_persistence_images, valid_labels_medical_check, test_labels_medical_check = train_test_split(\n",
    "    medical_abstract_persistence_images, medical_selected_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Sanity check: Ensure medical label splits are consistent\n",
    "assert np.array_equal(valid_labels_medical, valid_labels_medical_check), \"Validation labels do not match for Medical Laplacians and Abstract persistence images!\"\n",
    "assert np.array_equal(test_labels_medical, test_labels_medical_check), \"Test labels do not match for Medical Laplacians and Abstract persistence images!\"\n",
    "\n",
    "\n",
    "# Combine validation sets for Abstract Persistence Images and labels\n",
    "valid_abstract_persistence_images = valid_data_gudhi_abstract_persistence_images + valid_data_medical_abstract_persistence_images\n",
    "valid_abstract_labels = np.concatenate((valid_labels_gudhi, valid_labels_medical))\n",
    "\n",
    "# Combine test sets for Abstract Persistence Images and labels\n",
    "test_abstract_persistence_images = test_data_gudhi_abstract_persistence_images + test_data_medical_abstract_persistence_images\n",
    "test_abstract_labels = np.concatenate((test_labels_gudhi, test_labels_medical))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes=2):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Pooling Layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Adaptive Pooling to resize to 100x100\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((100, 100))\n",
    "        \n",
    "        # Dynamically calculate input size to fc1\n",
    "        self.feature_size = self._get_feature_size(input_shape)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(self.feature_size, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _get_feature_size(self, input_shape):\n",
    "        # Create a dummy input to calculate size after conv and pooling\n",
    "        dummy_input = torch.zeros(1, 1, *input_shape)\n",
    "        x = self.pool(F.relu(self.conv1(dummy_input)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # Apply adaptive pooling to get 100x100 size\n",
    "        x = self.adaptive_pool(x)\n",
    "        return x.numel()  # Number of elements after flattening\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # Apply adaptive pooling to resize to 100x100\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        # Flatten and pass through fully connected layers\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualInputCNN(nn.Module):\n",
    "    def __init__(self, input_shape1, input_shape2, num_classes=2):\n",
    "        super(DualInputCNN, self).__init__()\n",
    "\n",
    "        # Laplacian input path with additional pooling to reduce to 100x100\n",
    "        self.conv1_lap = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2_lap = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool_lap = nn.MaxPool2d(2, 2)  # Reduce spatial dimensions\n",
    "        self.adaptive_pool_lap = nn.AdaptiveAvgPool2d((100, 100))  # Resize to 100x100\n",
    "        \n",
    "        # Persistence image input path (no pooling)\n",
    "        self.conv1_pers = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2_pers = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.adaptive_pool_pers = nn.AdaptiveAvgPool2d((100, 100))  # Resize to 100x100\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 100 * 100 + 32 * 100 * 100, 128)  # Adjusted for 100x100 input\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Laplacians path (downsampling to 100x100)\n",
    "        x1 = F.relu(self.conv1_lap(x1))\n",
    "        x1 = self.pool_lap(x1)  # First pool: 250x250 -> 125x125\n",
    "        x1 = F.relu(self.conv2_lap(x1))\n",
    "        x1 = self.pool_lap(x1)  # Second pool: 125x125 -> 62x62\n",
    "        x1 = self.adaptive_pool_lap(x1)  # Resize to 100x100\n",
    "        \n",
    "        # Persistence images path (no pooling)\n",
    "        x2 = F.relu(self.conv1_pers(x2))\n",
    "        x2 = F.relu(self.conv2_pers(x2))\n",
    "        x2 = self.adaptive_pool_pers(x2)  # Ensure persistence images are 100x100\n",
    "        \n",
    "        # Concatenate along dim=1 (channels)\n",
    "        x = torch.cat((x1, x2), dim=1)  # Concatenates the outputs along the channel axis\n",
    "\n",
    "        # Flatten for fully connected layer\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch Datasets\n",
    "train_dataset_laplacians = ShapeDataset(train_data_gudhi_laplacians, train_labels_gudhi)\n",
    "valid_dataset_laplacians = ShapeDataset(valid_laplacians, valid_labels)\n",
    "test_dataset_laplacians = ShapeDataset(test_laplacians, test_labels)\n",
    "\n",
    "train_dataset_vr = ShapeDataset(train_data_gudhi_vr_persistence_images, train_labels_gudhi)\n",
    "valid_dataset_vr = ShapeDataset(valid_vr_persistence_images, valid_vr_labels)\n",
    "test_dataset_vr = ShapeDataset(test_vr_persistence_images, test_vr_labels)\n",
    "\n",
    "train_dataset_abstract = ShapeDataset(train_data_gudhi_abstract_persistence_images, train_labels_gudhi)\n",
    "valid_dataset_abstract = ShapeDataset(valid_abstract_persistence_images, valid_abstract_labels)\n",
    "test_dataset_abstract = ShapeDataset(test_abstract_persistence_images, test_abstract_labels)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader_laplacians = torch.utils.data.DataLoader(train_dataset_laplacians, batch_size=batch_size, shuffle=False)\n",
    "valid_loader_laplacians = torch.utils.data.DataLoader(valid_dataset_laplacians, batch_size=batch_size, shuffle=False)\n",
    "test_loader_laplacians = torch.utils.data.DataLoader(test_dataset_laplacians, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader_vr = torch.utils.data.DataLoader(train_dataset_vr, batch_size=batch_size, shuffle=False)\n",
    "valid_loader_vr = torch.utils.data.DataLoader(valid_dataset_vr, batch_size=batch_size, shuffle=False)\n",
    "test_loader_vr = torch.utils.data.DataLoader(test_dataset_vr, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader_abstract = torch.utils.data.DataLoader(train_dataset_abstract, batch_size=batch_size, shuffle=False)\n",
    "valid_loader_abstract = torch.utils.data.DataLoader(valid_dataset_abstract, batch_size=batch_size, shuffle=False)\n",
    "test_loader_abstract = torch.utils.data.DataLoader(test_dataset_abstract, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For single-input CNN (Laplacians)\n",
    "input_shape = train_data_gudhi_laplacians[0].shape\n",
    "num_classes = 2 # binary classification\n",
    "\n",
    "model_single_laplacians = CNN(input_shape, num_classes)\n",
    "\n",
    "# For dual-input CNNs (Laplacians + VR Persistence Images, Laplacians + Abstract Persistence Images)\n",
    "input_shape1 = train_data_gudhi_laplacians[0].shape\n",
    "input_shape2 = train_data_gudhi_vr_persistence_images[0].shape\n",
    "input_shape3 = train_data_gudhi_abstract_persistence_images[0].shape\n",
    "\n",
    "model_dual_lap_vr = DualInputCNN(input_shape1, input_shape2, num_classes)\n",
    "model_dual_lap_abstract = DualInputCNN(input_shape1, input_shape3, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_input(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        \n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return total_loss / len(dataloader.dataset), accuracy\n",
    "\n",
    "\n",
    "def validate_single_input(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Validating\", leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return total_loss / len(dataloader.dataset), accuracy\n",
    "\n",
    "\n",
    "def train_dual_input(model, dataloader1, dataloader2, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    progress_bar = tqdm(zip(dataloader1, dataloader2), desc=\"Training (Dual Input)\", leave=False, total=min(len(dataloader1), len(dataloader2)))\n",
    "    \n",
    "    for (inputs1, labels1), (inputs2, labels2) in progress_bar:\n",
    "        inputs1, labels1 = inputs1.to(device), labels1.to(device)\n",
    "        inputs2, labels2 = inputs2.to(device), labels2.to(device)\n",
    "        \n",
    "        if not torch.equal(labels1, labels2):\n",
    "            print(\"Labels mismatch in dual-input training! Skipping batch.\")\n",
    "            continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs1, inputs2)\n",
    "        loss = criterion(outputs, labels1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels1).sum().item()\n",
    "        \n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    accuracy = correct / len(dataloader1.dataset)\n",
    "    return total_loss / len(dataloader1.dataset), accuracy\n",
    "\n",
    "\n",
    "def validate_dual_input(model, valid_loader_laplacians, valid_loader_vr, criterion, device='cuda'):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (inputs1, _), (inputs2, targets) in zip(valid_loader_laplacians, valid_loader_vr):\n",
    "            inputs1, inputs2, targets = inputs1.to(device), inputs2.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs1, inputs2)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if outputs.shape[-1] > 1:\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "            else:  # Binary classification\n",
    "                predicted = (outputs > 0.5).float()\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(valid_loader_laplacians)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def test_single_input(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Testing\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Loss': [avg_loss],\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F1 Score': [f1]\n",
    "    })\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "def test_dual_input(model, dataloader1, dataloader2, criterion, device='cuda'):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    progress_bar = tqdm(zip(dataloader1, dataloader2), desc=\"Testing (Dual Input)\", leave=False, total=min(len(dataloader1), len(dataloader2)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs1, _), (inputs2, targets) in progress_bar:\n",
    "            inputs1, inputs2, targets = inputs1.to(device), inputs2.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs1, inputs2)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if outputs.shape[-1] > 1:\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "            else:  # Binary classification\n",
    "                predicted = (outputs > 0.5).float()\n",
    "\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader1.dataset)\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Loss': [avg_loss],\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F1 Score': [f1]\n",
    "    })\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DualInputCNN(\n",
       "  (conv1_lap): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_lap): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool_lap): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (adaptive_pool_lap): AdaptiveAvgPool2d(output_size=(100, 100))\n",
       "  (conv1_pers): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_pers): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (adaptive_pool_pers): AdaptiveAvgPool2d(output_size=(100, 100))\n",
       "  (fc1): Linear(in_features=640000, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_single_laplacians.to(device)\n",
    "model_dual_lap_vr.to(device)\n",
    "model_dual_lap_abstract.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_single = optim.Adam(model_single_laplacians.parameters(), lr=0.001)\n",
    "optimizer_dual_vr = optim.Adam(model_dual_lap_vr.parameters(), lr=0.001)\n",
    "optimizer_dual_abstract = optim.Adam(model_dual_lap_abstract.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfcf00fce21476a9934a1268fdf5c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53d7b50e6574ffda6b907a6b18cb255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288529a4c1164dae9310d05099ec35b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0540, Test Accuracy: 0.8897, Precision: 0.9439, Recall: 0.8016, F1 Score: 0.8670\n",
      "single-input model (Laplacians) Epoch 1/10, Train Loss: 0.0531, Train Acc: 0.8444, Valid Loss: 0.0560, Valid Acc: 0.8612\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070cf51437be42d791cd42e350d89bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced948c9ed304e2fb0c6a5af338b336f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d69eb613dd041a49f45db7040a8cdfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1029, Test Accuracy: 0.8754, Precision: 0.9099, Recall: 0.8016, F1 Score: 0.8523\n",
      "single-input model (Laplacians) Epoch 2/10, Train Loss: 0.0066, Train Acc: 0.9413, Valid Loss: 0.1055, Valid Acc: 0.8719\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db69ce24f82443b953b686f5b00dbae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c778e13bf1a499e86a7de5517a50c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ee33e1e8d548f9888ab67300772890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1255, Test Accuracy: 0.8968, Precision: 0.9619, Recall: 0.8016, F1 Score: 0.8745\n",
      "single-input model (Laplacians) Epoch 3/10, Train Loss: 0.0067, Train Acc: 0.9437, Valid Loss: 0.1289, Valid Acc: 0.8754\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee65f9a004348e09531a9b2b8a99885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f8a9985bf14c46bd4b19b0fb785064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecee8a3f85349d09f8d6252fa3d680e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1473, Test Accuracy: 0.8790, Precision: 0.9182, Recall: 0.8016, F1 Score: 0.8559\n",
      "single-input model (Laplacians) Epoch 4/10, Train Loss: 0.0052, Train Acc: 0.9513, Valid Loss: 0.1510, Valid Acc: 0.8754\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29f4dd8a22b4234b2851ba170c50a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea06608aac947b6951bb38b5cd6d309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23cd5a0684a4ed0a863741be7a89902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2099, Test Accuracy: 0.8897, Precision: 0.9612, Recall: 0.7857, F1 Score: 0.8646\n",
      "single-input model (Laplacians) Epoch 5/10, Train Loss: 0.0053, Train Acc: 0.9481, Valid Loss: 0.2151, Valid Acc: 0.8612\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dcfb6fdd6e4a49b082e980e3c0ee5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c00045bd4734e249dec172f7c1b0757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0607eae7ba44a2a9cd2e9a225f2831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1965, Test Accuracy: 0.8968, Precision: 0.9619, Recall: 0.8016, F1 Score: 0.8745\n",
      "single-input model (Laplacians) Epoch 6/10, Train Loss: 0.0051, Train Acc: 0.9537, Valid Loss: 0.2021, Valid Acc: 0.8754\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebd4c7a5603437abd23c80cd368e484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656d5baee712426fbf451450ef06ef1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e4ba44698544e49ab129d2065af69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2267, Test Accuracy: 0.8754, Precision: 0.9252, Recall: 0.7857, F1 Score: 0.8498\n",
      "single-input model (Laplacians) Epoch 7/10, Train Loss: 0.0051, Train Acc: 0.9569, Valid Loss: 0.2326, Valid Acc: 0.8612\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e848d033832f4bec9cfebe46740e34b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4612bfef87d146f0919d377cd3e58e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74afe10280a544b288232637ebf8a79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2153, Test Accuracy: 0.8968, Precision: 0.9619, Recall: 0.8016, F1 Score: 0.8745\n",
      "single-input model (Laplacians) Epoch 8/10, Train Loss: 0.0055, Train Acc: 0.9544, Valid Loss: 0.2209, Valid Acc: 0.8754\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872ccb5fde54432bb0ae31723983bff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5557d3a9501d4a548e6067bae42fff2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f0a80a430a4c0f94d30f714e20b12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1998, Test Accuracy: 0.8968, Precision: 0.9619, Recall: 0.8016, F1 Score: 0.8745\n",
      "single-input model (Laplacians) Epoch 9/10, Train Loss: 0.0059, Train Acc: 0.9475, Valid Loss: 0.2051, Valid Acc: 0.8719\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ce0c268a9f41629a545d52bee766a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f8e8b015c941beb2f1c7d6c0fd37ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30a30bc23b243d9a88033253956c79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2066, Test Accuracy: 0.8968, Precision: 0.9619, Recall: 0.8016, F1 Score: 0.8745\n",
      "single-input model (Laplacians) Epoch 10/10, Train Loss: 0.0052, Train Acc: 0.9569, Valid Loss: 0.2121, Valid Acc: 0.8719\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a76402400dc468da7460ec477d6d56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b6504b3a12468d92c2718bf7182bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0786, Test Accuracy: 0.8185, Precision: 0.7168, Recall: 0.9841, F1 Score: 0.8294\n",
      "dual-input model (Laplacians + VR Persistence Images) Epoch 1/10, Train Loss: 0.9655, Train Acc: 0.8569, Valid Loss: 2.1300, Valid Acc: 0.8327\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db6d93e2ac94eaaacc6ea389b8ba512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9db669176734c2a829f2a94666bf606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0524, Test Accuracy: 0.8114, Precision: 0.7135, Recall: 0.9683, F1 Score: 0.8215\n",
      "dual-input model (Laplacians + VR Persistence Images) Epoch 2/10, Train Loss: 0.0028, Train Acc: 0.9838, Valid Loss: 1.3202, Valid Acc: 0.8292\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8c75a48e4d49e8901532700ab7f852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341d00ff599340269edbd2480f91f802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0434, Test Accuracy: 0.8185, Precision: 0.7219, Recall: 0.9683, F1 Score: 0.8271\n",
      "dual-input model (Laplacians + VR Persistence Images) Epoch 3/10, Train Loss: 0.0017, Train Acc: 0.9888, Valid Loss: 1.0549, Valid Acc: 0.8292\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4622c1ab9d4a69a3dbebf652da3848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b17f8dff97e4e41bd8da32ff55f4275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0348, Test Accuracy: 0.8292, Precision: 0.7378, Recall: 0.9603, F1 Score: 0.8345\n",
      "dual-input model (Laplacians + VR Persistence Images) Epoch 4/10, Train Loss: 0.0013, Train Acc: 0.9856, Valid Loss: 0.7646, Valid Acc: 0.8434\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a158117a41554c9692fd0ac38282c093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25841340daf24199954f0b6020ee55c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0365, Test Accuracy: 0.8256, Precision: 0.7362, Recall: 0.9524, F1 Score: 0.8304\n",
      "dual-input model (Laplacians + VR Persistence Images) Epoch 5/10, Train Loss: 0.0007, Train Acc: 0.9906, Valid Loss: 0.8004, Valid Acc: 0.8399\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e87e003762e417c94109e56ae1e2084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d0f3a7a3ff4dd0bf8e88935ab175b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0319, Test Accuracy: 0.8185, Precision: 0.7389, Recall: 0.9206, F1 Score: 0.8198\n",
      "dual-input model (Laplacians + VR Persistence Images) Epoch 6/10, Train Loss: 0.0032, Train Acc: 0.9819, Valid Loss: 0.5420, Valid Acc: 0.8612\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc61ae690ff647558cfabbb910e9a038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993833ab92134c849131ef6973da6c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0382, Test Accuracy: 0.8114, Precision: 0.7160, Recall: 0.9603, F1 Score: 0.8203\n",
      "dual-input model (Laplacians + VR Persistence Images) Epoch 7/10, Train Loss: 0.0039, Train Acc: 0.9719, Valid Loss: 0.8732, Valid Acc: 0.8363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c81cdce353d4502989f00a227a48234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ea83b8a22545a7b5fde038bb81429e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0379, Test Accuracy: 0.8185, Precision: 0.7358, Recall: 0.9286, F1 Score: 0.8211\n",
      "dual-input model (Laplacians + VR Persistence Images) Epoch 8/10, Train Loss: 0.0011, Train Acc: 0.9862, Valid Loss: 0.8149, Valid Acc: 0.8434\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a1b750b2c5426ba8e276c63cb4954d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9380fe168048d4b6e7677b2fad2eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0487, Test Accuracy: 0.8292, Precision: 0.7378, Recall: 0.9603, F1 Score: 0.8345\n",
      "dual-input model (Laplacians + VR Persistence Images) Epoch 9/10, Train Loss: 0.0007, Train Acc: 0.9938, Valid Loss: 1.1236, Valid Acc: 0.8399\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bd1ba2dfa542d59fa9eb34a7e91a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ab5789c60d4d94b3c6659a6edcf613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0625, Test Accuracy: 0.8078, Precision: 0.7069, Recall: 0.9762, F1 Score: 0.8200\n",
      "dual-input model (Laplacians + VR Persistence Images) Epoch 10/10, Train Loss: 0.0010, Train Acc: 0.9925, Valid Loss: 1.5710, Valid Acc: 0.8292\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920888713bf44ae7ac6b3f7d20e177c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff30fca89264a2e9327d4e1bf963231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1512, Test Accuracy: 0.8897, Precision: 1.0000, Recall: 0.7540, F1 Score: 0.8597\n",
      "dual-input model (Laplacians + Abstract Persistence Images) Epoch 1/10, Train Loss: 0.2715, Train Acc: 0.8725, Valid Loss: 4.7810, Valid Acc: 0.8861\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5653efdded4652ac5ad1c3d36f4a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39b91625e7f4cedb1630f2f08775868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1626, Test Accuracy: 0.8968, Precision: 0.9802, Recall: 0.7857, F1 Score: 0.8722\n",
      "dual-input model (Laplacians + Abstract Persistence Images) Epoch 2/10, Train Loss: 0.0017, Train Acc: 0.9781, Valid Loss: 5.0932, Valid Acc: 0.9039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e90014511448618e44dbac53c0e927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f4bd1d1f524b6f972f8491854fa0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1393, Test Accuracy: 0.8968, Precision: 0.9802, Recall: 0.7857, F1 Score: 0.8722\n",
      "dual-input model (Laplacians + Abstract Persistence Images) Epoch 3/10, Train Loss: 0.0013, Train Acc: 0.9844, Valid Loss: 4.3413, Valid Acc: 0.9039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec81c25312ee4beaa691af7e23f4f6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c837987698fc47d4994355695448b47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1481, Test Accuracy: 0.8968, Precision: 0.9802, Recall: 0.7857, F1 Score: 0.8722\n",
      "dual-input model (Laplacians + Abstract Persistence Images) Epoch 4/10, Train Loss: 0.0014, Train Acc: 0.9862, Valid Loss: 4.6294, Valid Acc: 0.9039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d034b5166464bafa3cc90ff45dca6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207aa1a38aa74d929144ae7013c729d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1698, Test Accuracy: 0.8968, Precision: 0.9709, Recall: 0.7937, F1 Score: 0.8734\n",
      "dual-input model (Laplacians + Abstract Persistence Images) Epoch 5/10, Train Loss: 0.0011, Train Acc: 0.9881, Valid Loss: 5.2995, Valid Acc: 0.9004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9956bbdbf1eb4eb7b707e9c544e2183f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4987d95abd4291b781fef50a5027d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1681, Test Accuracy: 0.9004, Precision: 0.9804, Recall: 0.7937, F1 Score: 0.8772\n",
      "dual-input model (Laplacians + Abstract Persistence Images) Epoch 6/10, Train Loss: 0.0010, Train Acc: 0.9900, Valid Loss: 5.2732, Valid Acc: 0.9039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fc90c99ef542809a1e1115d906275b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa27c87bd604cbfb387f58f8e09bdca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1760, Test Accuracy: 0.9004, Precision: 0.9804, Recall: 0.7937, F1 Score: 0.8772\n",
      "dual-input model (Laplacians + Abstract Persistence Images) Epoch 7/10, Train Loss: 0.0007, Train Acc: 0.9912, Valid Loss: 5.5024, Valid Acc: 0.9039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe5fa88d18c4bdca040342a5d2845d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ebb4a3de2f4d9a93d5df5848de6552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1699, Test Accuracy: 0.8968, Precision: 0.9709, Recall: 0.7937, F1 Score: 0.8734\n",
      "dual-input model (Laplacians + Abstract Persistence Images) Epoch 8/10, Train Loss: 0.0008, Train Acc: 0.9925, Valid Loss: 5.2558, Valid Acc: 0.9039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b98bf617514da491e0b192ce149d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91727e15d6784b7a8de3c9ad3d0e1bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1653, Test Accuracy: 0.9004, Precision: 0.9804, Recall: 0.7937, F1 Score: 0.8772\n",
      "dual-input model (Laplacians + Abstract Persistence Images) Epoch 9/10, Train Loss: 0.0008, Train Acc: 0.9919, Valid Loss: 5.0739, Valid Acc: 0.9004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17a7295e38344978cfde6912293dc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Dual Input):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c3a1d465fa423192d0b9389dbc0738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing (Dual Input):   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1545, Test Accuracy: 0.8932, Precision: 0.9528, Recall: 0.8016, F1 Score: 0.8707\n",
      "dual-input model (Laplacians + Abstract Persistence Images) Epoch 10/10, Train Loss: 0.0007, Train Acc: 0.9900, Valid Loss: 4.5546, Valid Acc: 0.9004\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to store all metrics for each epoch\n",
    "test_metrics_single_list = []\n",
    "test_metrics_dual_vr_list = []\n",
    "test_metrics_dual_abstract_list = []\n",
    "\n",
    "# Training loop for single-input model (Laplacians)\n",
    "for epoch in range(num_epochs):\n",
    "    model_single_laplacians.train()  # Ensure the model is in training mode\n",
    "    train_loss, train_acc = train_single_input(model_single_laplacians, train_loader_laplacians, optimizer_single, criterion, device)\n",
    "    model_single_laplacians.eval()  # Switch model to evaluation mode after training\n",
    "    valid_loss, valid_acc = validate_single_input(model_single_laplacians, valid_loader_laplacians, criterion, device)\n",
    "\n",
    "    # Test the model after each epoch\n",
    "    test_metrics_single = test_single_input(model_single_laplacians, test_loader_laplacians, criterion, device)\n",
    "    \n",
    "    # Combine train, valid, and test metrics for this epoch\n",
    "    metrics = test_metrics_single.copy()\n",
    "    metrics['Train Loss'] = train_loss\n",
    "    metrics['Train Accuracy'] = train_acc\n",
    "    metrics['Valid Loss'] = valid_loss\n",
    "    metrics['Valid Accuracy'] = valid_acc\n",
    "    \n",
    "    # Append to the list\n",
    "    test_metrics_single_list.append(metrics)\n",
    "\n",
    "    print(f\"single-input model (Laplacians) Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}\")\n",
    "\n",
    "torch.save(model_single_laplacians.state_dict(), \"model_single_laplacians_final.pth\")\n",
    "\n",
    "# Training loop for dual-input model (Laplacians + VR Persistence Images)\n",
    "for epoch in range(num_epochs):\n",
    "    model_dual_lap_vr.train()  # Ensure the model is in training mode\n",
    "    train_loss, train_acc = train_dual_input(model_dual_lap_vr, train_loader_laplacians, train_loader_vr, optimizer_dual_vr, criterion, device)\n",
    "    model_dual_lap_vr.eval()  # Switch model to evaluation mode after training\n",
    "    valid_loss, valid_acc = validate_dual_input(model_dual_lap_vr, valid_loader_laplacians, valid_loader_vr, criterion, device)\n",
    "\n",
    "    # Test the model after each epoch\n",
    "    test_metrics_dual_vr = test_dual_input(model_dual_lap_vr, test_loader_laplacians, test_loader_vr, criterion, device)\n",
    "    \n",
    "    # Combine train, valid, and test metrics for this epoch\n",
    "    metrics = test_metrics_dual_vr.copy()\n",
    "    metrics['Train Loss'] = train_loss\n",
    "    metrics['Train Accuracy'] = train_acc\n",
    "    metrics['Valid Loss'] = valid_loss\n",
    "    metrics['Valid Accuracy'] = valid_acc\n",
    "    \n",
    "    # Append to the list\n",
    "    test_metrics_dual_vr_list.append(metrics)\n",
    "\n",
    "    print(f\"dual-input model (Laplacians + VR Persistence Images) Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}\")\n",
    "\n",
    "torch.save(model_dual_lap_vr.state_dict(), \"model_dual_lap_vr_final.pth\")\n",
    "\n",
    "# Training loop for dual-input model (Laplacians + Abstract Persistence Images)\n",
    "for epoch in range(num_epochs):\n",
    "    model_dual_lap_abstract.train()  # Ensure the model is in training mode\n",
    "    train_loss, train_acc = train_dual_input(model_dual_lap_abstract, train_loader_laplacians, train_loader_abstract, optimizer_dual_abstract, criterion, device)\n",
    "    model_dual_lap_abstract.eval()  # Switch model to evaluation mode after training\n",
    "    valid_loss, valid_acc = validate_dual_input(model_dual_lap_abstract, valid_loader_laplacians, valid_loader_abstract, criterion, device)\n",
    "\n",
    "    # Test the model after each epoch\n",
    "    test_metrics_dual_abstract = test_dual_input(model_dual_lap_abstract, test_loader_laplacians, test_loader_abstract, criterion, device)\n",
    "    \n",
    "    # Combine train, valid, and test metrics for this epoch\n",
    "    metrics = test_metrics_dual_abstract.copy()\n",
    "    metrics['Train Loss'] = train_loss\n",
    "    metrics['Train Accuracy'] = train_acc\n",
    "    metrics['Valid Loss'] = valid_loss\n",
    "    metrics['Valid Accuracy'] = valid_acc\n",
    "    \n",
    "    # Append to the list\n",
    "    test_metrics_dual_abstract_list.append(metrics)\n",
    "\n",
    "    print(f\"dual-input model (Laplacians + Abstract Persistence Images) Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}\")\n",
    "\n",
    "torch.save(model_dual_lap_abstract.state_dict(), \"model_dual_lap_abstract_final.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single-input model (Laplacians) Test Metrics:\n",
      "       Test Loss  Test Accuracy  Precision    Recall  F1 Score  Train Loss  \\\n",
      "Epoch                                                                        \n",
      "1       0.053981       0.889680   0.943925  0.801587  0.866953    0.053081   \n",
      "2       0.102899       0.875445   0.909910  0.801587  0.852321    0.006592   \n",
      "3       0.125530       0.896797   0.961905  0.801587  0.874459    0.006663   \n",
      "4       0.147281       0.879004   0.918182  0.801587  0.855932    0.005214   \n",
      "5       0.209872       0.889680   0.961165  0.785714  0.864629    0.005348   \n",
      "6       0.196451       0.896797   0.961905  0.801587  0.874459    0.005096   \n",
      "7       0.226710       0.875445   0.925234  0.785714  0.849785    0.005122   \n",
      "8       0.215337       0.896797   0.961905  0.801587  0.874459    0.005464   \n",
      "9       0.199815       0.896797   0.961905  0.801587  0.874459    0.005937   \n",
      "10      0.206587       0.896797   0.961905  0.801587  0.874459    0.005184   \n",
      "\n",
      "       Train Accuracy  Valid Loss  Valid Accuracy  \n",
      "Epoch                                              \n",
      "1            0.844375    0.056037        0.861210  \n",
      "2            0.941250    0.105452        0.871886  \n",
      "3            0.943750    0.128872        0.875445  \n",
      "4            0.951250    0.150991        0.875445  \n",
      "5            0.948125    0.215086        0.861210  \n",
      "6            0.953750    0.202063        0.875445  \n",
      "7            0.956875    0.232572        0.861210  \n",
      "8            0.954375    0.220920        0.875445  \n",
      "9            0.947500    0.205103        0.871886  \n",
      "10           0.956875    0.212094        0.871886  \n",
      "\n",
      "Dual-input model (Laplacians + VR Persistence Images) Test Metrics:\n",
      "       Test Loss  Test Accuracy  Precision    Recall  F1 Score  Train Loss  \\\n",
      "Epoch                                                                        \n",
      "1       0.078643       0.818505   0.716763  0.984127  0.829431    0.965496   \n",
      "2       0.052408       0.811388   0.713450  0.968254  0.821549    0.002843   \n",
      "3       0.043413       0.818505   0.721893  0.968254  0.827119    0.001655   \n",
      "4       0.034799       0.829181   0.737805  0.960317  0.834483    0.001279   \n",
      "5       0.036463       0.825623   0.736196  0.952381  0.830450    0.000715   \n",
      "6       0.031892       0.818505   0.738854  0.920635  0.819788    0.003210   \n",
      "7       0.038243       0.811388   0.715976  0.960317  0.820339    0.003918   \n",
      "8       0.037938       0.818505   0.735849  0.928571  0.821053    0.001140   \n",
      "9       0.048668       0.829181   0.737805  0.960317  0.834483    0.000735   \n",
      "10      0.062459       0.807829   0.706897  0.976190  0.820000    0.000959   \n",
      "\n",
      "       Train Accuracy  Valid Loss  Valid Accuracy  \n",
      "Epoch                                              \n",
      "1            0.856875    2.129993        0.832740  \n",
      "2            0.983750    1.320240        0.829181  \n",
      "3            0.988750    1.054949        0.829181  \n",
      "4            0.985625    0.764594        0.843416  \n",
      "5            0.990625    0.800424        0.839858  \n",
      "6            0.981875    0.541957        0.861210  \n",
      "7            0.971875    0.873151        0.836299  \n",
      "8            0.986250    0.814926        0.843416  \n",
      "9            0.993750    1.123557        0.839858  \n",
      "10           0.992500    1.570964        0.829181  \n",
      "\n",
      "Dual-input model (Laplacians + Abstract Persistence Images) Test Metrics:\n",
      "       Test Loss  Test Accuracy  Precision    Recall  F1 Score  Train Loss  \\\n",
      "Epoch                                                                        \n",
      "1       0.151234       0.889680   1.000000  0.753968  0.859729    0.271531   \n",
      "2       0.162589       0.896797   0.980198  0.785714  0.872247    0.001696   \n",
      "3       0.139324       0.896797   0.980198  0.785714  0.872247    0.001283   \n",
      "4       0.148068       0.896797   0.980198  0.785714  0.872247    0.001356   \n",
      "5       0.169787       0.896797   0.970874  0.793651  0.873362    0.001123   \n",
      "6       0.168143       0.900356   0.980392  0.793651  0.877193    0.001041   \n",
      "7       0.175968       0.900356   0.980392  0.793651  0.877193    0.000700   \n",
      "8       0.169900       0.896797   0.970874  0.793651  0.873362    0.000831   \n",
      "9       0.165331       0.900356   0.980392  0.793651  0.877193    0.000774   \n",
      "10      0.154510       0.893238   0.952830  0.801587  0.870690    0.000733   \n",
      "\n",
      "       Train Accuracy  Valid Loss  Valid Accuracy  \n",
      "Epoch                                              \n",
      "1            0.872500    4.780969        0.886121  \n",
      "2            0.978125    5.093243        0.903915  \n",
      "3            0.984375    4.341265        0.903915  \n",
      "4            0.986250    4.629427        0.903915  \n",
      "5            0.988125    5.299521        0.900356  \n",
      "6            0.990000    5.273211        0.903915  \n",
      "7            0.991250    5.502392        0.903915  \n",
      "8            0.992500    5.255775        0.903915  \n",
      "9            0.991875    5.073878        0.900356  \n",
      "10           0.990000    4.554590        0.900356  \n"
     ]
    }
   ],
   "source": [
    "# Convert the lists of metrics into DataFrames with epoch as the index\n",
    "test_metrics_single_df = pd.concat(test_metrics_single_list, ignore_index=True)\n",
    "test_metrics_dual_vr_df = pd.concat(test_metrics_dual_vr_list, ignore_index=True)\n",
    "test_metrics_dual_abstract_df = pd.concat(test_metrics_dual_abstract_list, ignore_index=True)\n",
    "\n",
    "# Add 'Epoch' as new column\n",
    "epochs = list(range(1, num_epochs + 1))\n",
    "\n",
    "test_metrics_single_df['Epoch'] = epochs\n",
    "test_metrics_dual_vr_df['Epoch'] = epochs\n",
    "test_metrics_dual_abstract_df['Epoch'] = epochs\n",
    "\n",
    "# Set 'Epoch' as index\n",
    "test_metrics_single_df.set_index('Epoch', inplace=True)\n",
    "test_metrics_dual_vr_df.set_index('Epoch', inplace=True)\n",
    "test_metrics_dual_abstract_df.set_index('Epoch', inplace=True)\n",
    "\n",
    "# Rename first two columns to 'Test Loss' and 'Test Accuracy'\n",
    "test_metrics_single_df.rename(columns={'Loss': 'Test Loss', 'Accuracy': 'Test Accuracy'}, inplace=True)\n",
    "test_metrics_dual_vr_df.rename(columns={'Loss': 'Test Loss', 'Accuracy': 'Test Accuracy'}, inplace=True)\n",
    "test_metrics_dual_abstract_df.rename(columns={'Loss': 'Test Loss', 'Accuracy': 'Test Accuracy'}, inplace=True)\n",
    "\n",
    "# Print final test metrics for all models\n",
    "print(\"\\nSingle-input model (Laplacians) Test Metrics:\")\n",
    "print(test_metrics_single_df)\n",
    "\n",
    "print(\"\\nDual-input model (Laplacians + VR Persistence Images) Test Metrics:\")\n",
    "print(test_metrics_dual_vr_df)\n",
    "\n",
    "print(\"\\nDual-input model (Laplacians + Abstract Persistence Images) Test Metrics:\")\n",
    "print(test_metrics_dual_abstract_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export test metrics to CSV files\n",
    "\n",
    "test_metrics_single_df.to_csv('test_metrics_single_laplacian_gudhi_medical.csv')\n",
    "test_metrics_dual_vr_df.to_csv('test_metrics_dual_lap_vr_gudhi_medical.csv')\n",
    "test_metrics_dual_abstract_df.to_csv('test_metrics_dual_lap_abstract_gudhi_medical.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FiltrationsForGNNsLargeData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
